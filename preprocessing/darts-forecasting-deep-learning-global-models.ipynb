{"cells":[{"cell_type":"markdown","metadata":{},"source":["<img style=\"float: center;\" src=\"https://unit8.com/wp-content/uploads/2021/07/darts-logo2-01-scaled.jpg\" width=\"210\">"]},{"cell_type":"markdown","metadata":{},"source":["# **TL;DR:** \n","   \n","    \n","ðŸ“Œ **<ins>Baseline</ins>**: \n","    \n","* **Exponential Smoothing** applied to every single series generates a good baseline forecast - **RMSLE: 0.40578**\n"," <br />\n","    \n","ðŸ“Œ **<ins>Goal</ins>**: \n","\n","* Leverage the large amount of timeseries via **global models** with more complex, flexible methods **(Boosted Trees, Neural Networks)**\n"," <br />   \n"," \n","ðŸ“Œ **<ins>Deep Learning Models</ins>**: \n","\n","* This notebook implements the Neural Network models **N-HiTS, LSTM and TFT** from Pytorch via the TS library Darts\n"," <br />   \n","    \n","ðŸ“Œ **<ins>Best Model</ins>**: \n","    \n","* **33 Global LightGBM Models**, one for every product family\n","* **Each trained on 54 timeseries** (for 54 stores)\n","\n","ðŸ† ->   **RMSLE: 0.38558**  ->  #1 Leaderboard (21.09.2022), V24 of this notebook\n","    <br />  \n","    <br />  \n","    <br />  \n","    \n","    \n","\n","## What's happening here?\n","   <br />\n","\n","In this notebook, I try to test and learn different approaches towards using Machine Learning for Timeseries Forecasting. I want to present a comprehensive forecasting workflow. My main focus is on exploring Neural Network models (such as LSTM, NBEATS, TCN, TFT, N-HiTS).\n","\n","My basic understanding is: such complex and flexible methods require a lot of data in order to perform well. With single, univariate timeseries, that's usually not the case, and statistical methods with more structure tend to perform better. Here however we have 1782 parallel and related timeseries - Sales data for 33 product categories in 54 stores.\n","\n","Approaches I want to experiment with:\n","\n","* **Global Models (trained on multiple timeseries)**\n","* **Hierarchical Forecasting (Forecast Reconciliation)**\n","* **Ensembling**\n","* **Combinations of all the above**\n","\n","I use the **<ins>Darts Library</ins>** for my timeseries modeling - it simplifies the workflow a lot for a beginner-level programmer like me and has implementations of all the newest Deep Learning Forecasting methods around.\n","\n","So far however, the simple Exponential Smoothing baseline proved superior to my (global) Neural Network models. Global Models based on boosted trees however perform well - so far the best. But I'm sure there is still a lot of room for optimization.\n","\n","*I spammed the leaderboard with many submissions, as I couldn't replicate my original LightGBM submission created on Google Colab - turns out I had one little error in my code here when creating the final submission file with zero forecasting. I cleaned up that error now, so everything should be reproducible."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"1\"></a> <br>\n","# 1. Libraries"]},{"cell_type":"markdown","metadata":{},"source":["I use the [**Darts library**](https://unit8co.github.io/darts/index.html) for all my modeling here. It is an excellent and intuitive option for forecasting in Python, especially with regards to NN models. The developing team is very helpful and answering questions in their public communication channels, so I can highly recommend the library for timeseries forecasting!"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-11-17T01:14:21.69721Z","iopub.status.busy":"2022-11-17T01:14:21.696415Z","iopub.status.idle":"2022-11-17T01:15:22.039168Z","shell.execute_reply":"2022-11-17T01:15:22.037994Z","shell.execute_reply.started":"2022-11-17T01:14:21.697136Z"},"trusted":true},"outputs":[],"source":["# DARTS Library for Forecasting\n","\n","#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n","#!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchtext==0.10.0 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n","\n","    !pip install pyyaml==5.4.1\n","    !pip install darts\n","    import darts\n","    print(darts.__version__)\n","\n","    !pip install -U optuna==2.0.0\n","\n","    #!pip install --upgrade numpy\n","    import numpy as np\n","    import time\n","\n","    from darts import TimeSeries\n","    from darts.utils.timeseries_generation import gaussian_timeseries, linear_timeseries, sine_timeseries\n","    from darts.models import LightGBMModel, CatBoostModel, Prophet, RNNModel, TFTModel, NaiveSeasonal, ExponentialSmoothing, NHiTSModel\n","    from darts.metrics import mape, smape, rmse, rmsle\n","    from darts.dataprocessing import Pipeline\n","    from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\n","    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n","    from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis, plot_hist\n","    from darts.utils.likelihood_models import QuantileRegression\n","    from darts.utils.missing_values import fill_missing_values\n","    from darts.models import MovingAverage\n","\n","    import optuna\n","    from optuna.integration import PyTorchLightningPruningCallback\n","    from optuna.visualization import (\n","        plot_optimization_history,\n","        plot_contour,\n","        plot_param_importances,\n","    )\n","\n","    from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","\n","    from tqdm import tqdm\n","\n","    import sklearn\n","    from sklearn import preprocessing\n","\n","    import pandas as pd\n","    import torch\n","    import matplotlib.pyplot as plt\n","    import gc\n","\n","    %matplotlib inline\n","    torch.manual_seed(1); np.random.seed(1)  # for reproducibility"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"2\"></a> <br>\n","# 2. Data"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"2.1.\"></a> <br>\n","# 2.1. Pre-Processing"]},{"cell_type":"markdown","metadata":{},"source":["After loading the data, I create **Darts-specific TimeSeries objects**. For the sales data, I generate so-called static covariates for each series (store number, product family, city, region, type and cluster). Those might be used by some models in Darts later on. I also create a set of time-based covariates like weekday, month and year. All covariate series get stacked together.\n","\n","Furthermore, I scale all series between 0 and 1, and then apply a logarithmic transformation to them. I took this idea from [this helpful notebook](https://www.kaggle.com/code/carlmcbrideellis/store-sales-using-the-average-of-the-last-16-days). "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-11-17T01:15:22.042547Z","iopub.status.busy":"2022-11-17T01:15:22.041607Z","iopub.status.idle":"2022-11-17T01:21:46.49557Z","shell.execute_reply":"2022-11-17T01:21:46.494364Z","shell.execute_reply.started":"2022-11-17T01:15:22.042514Z"},"trusted":true},"outputs":[],"source":["# Load all Datasets\n","df_train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv')\n","df_test = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv')\n","df_holidays_events = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv')\n","df_oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv')\n","df_stores = pd.read_csv('../input/store-sales-time-series-forecasting/stores.csv')\n","df_transactions = pd.read_csv('../input/store-sales-time-series-forecasting/transactions.csv')\n","df_sample_submission = pd.read_csv('../input/store-sales-time-series-forecasting/sample_submission.csv')\n","\n","# Sales Data (Target)\n","\n","family_list = df_train['family'].unique()\n","family_list\n","store_list = df_stores['store_nbr'].unique()\n","store_list\n","\n","train_merged = pd.merge(df_train, df_stores, on ='store_nbr')\n","train_merged = train_merged.sort_values([\"store_nbr\",\"family\",\"date\"])\n","train_merged = train_merged.astype({\"store_nbr\":'str', \"family\":'str', \"city\":'str',\n","                          \"state\":'str', \"type\":'str', \"cluster\":'str'})\n","\n","df_test_dropped = df_test.drop(['onpromotion'], axis=1)\n","df_test_sorted = df_test_dropped.sort_values(by=['store_nbr','family'])\n","\n","# Create TimeSeries objects (Darts) and arrange in a Dictionary clustered by Product Family\n","\n","family_TS_dict = {}\n","\n","for family in family_list:\n","  df_family = train_merged.loc[train_merged['family'] == family]\n","\n","  list_of_TS_family = TimeSeries.from_group_dataframe(\n","                                df_family,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n","                                value_cols=\"sales\", # target variable\n","                                fill_missing_dates=True,\n","                                freq='D')\n","  for ts in list_of_TS_family:\n","            ts = ts.astype(np.float32)\n","\n","  list_of_TS_family = sorted(list_of_TS_family, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n","  family_TS_dict[family] = list_of_TS_family\n","\n","# Transform the Sales Data\n","\n","family_pipeline_dict = {}\n","family_TS_transformed_dict = {}\n","\n","for key in family_TS_dict:\n","  train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","  static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") #OneHotEncoder would be better but takes longer\n","  log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","  train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","  train_pipeline = Pipeline([train_filler,\n","                             static_cov_transformer,\n","                             log_transformer,\n","                             train_scaler])\n","     \n","  training_transformed = train_pipeline.fit_transform(family_TS_dict[key])\n","  family_pipeline_dict[key] = train_pipeline\n","  family_TS_transformed_dict[key] = training_transformed\n","\n","# Create TimeSeries objects (Darts) 1782\n","\n","list_of_TS = TimeSeries.from_group_dataframe(\n","                                train_merged,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n","                                value_cols=\"sales\", # target variable\n","                                fill_missing_dates=True,\n","                                freq='D')\n","for ts in list_of_TS:\n","            ts = ts.astype(np.float32)\n","\n","list_of_TS = sorted(list_of_TS, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n","\n","# Transform the Sales Data\n","\n","train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") #OneHotEncoder would be better but takes longer\n","log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","train_pipeline = Pipeline([train_filler,\n","                             static_cov_transformer,\n","                             log_transformer,\n","                             train_scaler])\n","     \n","training_transformed = train_pipeline.fit_transform(list_of_TS)\n","\n","#train_merged.head()\n","\n","# Create 7-day and 28-day moving average of sales\n","\n","sales_moving_average_7 = MovingAverage(window=7)\n","sales_moving_average_28 = MovingAverage(window=28)\n","\n","sales_moving_averages_dict = {}\n","\n","for key in family_TS_transformed_dict:\n","  sales_mas_family = []\n","  \n","  for ts in family_TS_transformed_dict[key]:\n","    ma_7 = sales_moving_average_7.filter(ts)\n","    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n","    ma_7 = ma_7.astype(np.float32)\n","    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"sales_ma_7\")\n","    ma_28 = sales_moving_average_28.filter(ts)\n","    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n","    ma_28 = ma_28.astype(np.float32)\n","    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"sales_ma_28\")\n","    mas = ma_7.stack(ma_28)\n","    sales_mas_family.append(mas)\n","  \n","  sales_moving_averages_dict[key] = sales_mas_family  \n","    \n","# General Covariates (Time-Based and Oil)\n","\n","full_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\n","\n","# Time-Based Covariates\n","\n","year = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"year\")\n","month = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"month\")\n","day = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"day\")\n","dayofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofyear\")\n","weekday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofweek\")\n","weekofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"weekofyear\")\n","timesteps = TimeSeries.from_times_and_values(times=full_time_period,\n","                                             values=np.arange(len(full_time_period)),\n","                                             columns=[\"linear_increase\"])\n","\n","time_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\n","time_cov = time_cov.astype(np.float32)\n","\n","# Transform\n","time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","time_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\n","time_cov_scaler.fit(time_cov_train)\n","time_cov_transformed = time_cov_scaler.transform(time_cov)\n","\n","#time_cov_transformed[-50:].plot()\n","\n","# Oil Price\n","\n","oil = TimeSeries.from_dataframe(df_oil, \n","                                time_col = 'date', \n","                                value_cols = ['dcoilwtico'],\n","                                freq = 'D')\n","\n","oil = oil.astype(np.float32)\n","\n","# Transform\n","oil_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","oil_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","oil_pipeline = Pipeline([oil_filler, oil_scaler])\n","oil_transformed = oil_pipeline.fit_transform(oil)\n","\n","# Moving Averages for Oil Price\n","oil_moving_average_7 = MovingAverage(window=7)\n","oil_moving_average_28 = MovingAverage(window=28)\n","\n","oil_moving_averages = []\n","\n","ma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\n","ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\n","ma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\n","ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\n","oil_moving_averages = ma_7.stack(ma_28)\n","\n","# Stack General Covariates Together\n","\n","general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)\n","\n","# Store-Specific Covariates (Transactions and Holidays)\n","\n","# Transactions\n","df_transactions.sort_values([\"store_nbr\",\"date\"], inplace=True)\n","\n","TS_transactions_list = TimeSeries.from_group_dataframe(\n","                                df_transactions,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                value_cols=\"transactions\",\n","                                fill_missing_dates=True,\n","                                freq='D')\n","\n","transactions_list = []\n","\n","for ts in TS_transactions_list:\n","            series = TimeSeries.from_series(ts.pd_series())   # necessary workaround to remove static covariates (so I can stack covariates later on)\n","            series = series.astype(np.float32)\n","            transactions_list.append(series)\n","\n","transactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\n","\n","from datetime import datetime, timedelta\n","\n","transactions_list_full = []\n","\n","for ts in transactions_list:\n","  if ts.start_time() > pd.Timestamp('20130101'):\n","    end_time = (ts.start_time() - timedelta(days=1))\n","    delta = end_time - pd.Timestamp('20130101')\n","    zero_series = TimeSeries.from_times_and_values(\n","                              times=pd.date_range(start=pd.Timestamp('20130101'), \n","                              end=end_time, freq=\"D\"),\n","                              values=np.zeros(delta.days+1))\n","    ts = zero_series.append(ts)\n","    transactions_list_full.append(ts)\n","\n","transactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","transactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","\n","transactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\n","transactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)\n","\n","# Moving Averages for Transactions\n","trans_moving_average_7 = MovingAverage(window=7)\n","trans_moving_average_28 = MovingAverage(window=28)\n","\n","transactions_covs = []\n","\n","for ts in transactions_transformed:\n","  ma_7 = trans_moving_average_7.filter(ts).astype(np.float32)\n","  ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"transactions_ma_7\")\n","  ma_28 = trans_moving_average_28.filter(ts).astype(np.float32)\n","  ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"transactions_ma_28\")\n","  trans_and_mas = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\").stack(ma_7).stack(ma_28)\n","  transactions_covs.append(trans_and_mas)\n","\n","\n","\n","# Re-Defining Categories of Holidays in a Meaningful Way\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['transferred'] == True,'Transferred', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Transfer','Holiday', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Additional','Holiday', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Bridge','Holiday', \n","                                      df_holidays_events['type'])\n","\n","\n","# Assign Holidays to all TimeSeries and Save in Dictionary\n","\n","def holiday_list(df_stores):\n","\n","    listofseries = []\n","    \n","    for i in range(0,len(df_stores)):\n","            \n","            df_holiday_dummies = pd.DataFrame(columns=['date'])\n","            df_holiday_dummies[\"date\"] = df_holidays_events[\"date\"]\n","            \n","            df_holiday_dummies[\"national_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & (df_holidays_events[\"locale\"] == \"National\")), 1, 0)\n","\n","            df_holiday_dummies[\"earthquake_relief\"] = np.where(df_holidays_events['description'].str.contains('Terremoto Manabi'), 1, 0)\n","\n","            df_holiday_dummies[\"christmas\"] = np.where(df_holidays_events['description'].str.contains('Navidad'), 1, 0)\n","\n","            df_holiday_dummies[\"football_event\"] = np.where(df_holidays_events['description'].str.contains('futbol'), 1, 0)\n","\n","            df_holiday_dummies[\"national_event\"] = np.where(((df_holidays_events[\"type\"] == \"Event\") & (df_holidays_events[\"locale\"] == \"National\") & (~df_holidays_events['description'].str.contains('Terremoto Manabi')) & (~df_holidays_events['description'].str.contains('futbol'))), 1, 0)\n","\n","            df_holiday_dummies[\"work_day\"] = np.where((df_holidays_events[\"type\"] == \"Work Day\"), 1, 0)\n","\n","            df_holiday_dummies[\"local_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & ((df_holidays_events[\"locale_name\"] == df_stores['state'][i]) | (df_holidays_events[\"locale_name\"] == df_stores['city'][i]))), 1, 0)\n","                     \n","            listofseries.append(df_holiday_dummies)\n","\n","    return listofseries\n","\n","def remove_0_and_duplicates(holiday_list):\n","\n","    listofseries = []\n","    \n","    for i in range(0,len(holiday_list)):\n","            \n","            df_holiday_per_store = list_of_holidays_per_store[i].set_index('date')\n","\n","            df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store==0).all(axis=1)]\n","            \n","            df_holiday_per_store = df_holiday_per_store.groupby('date').agg({'national_holiday':'max', 'earthquake_relief':'max', \n","                                   'christmas':'max', 'football_event':'max', \n","                                   'national_event':'max', 'work_day':'max', \n","                                   'local_holiday':'max'}).reset_index()\n","\n","            listofseries.append(df_holiday_per_store)\n","\n","    return listofseries\n","\n","def holiday_TS_list_54(holiday_list):\n","\n","    listofseries = []\n","    \n","    for i in range(0,54):\n","            \n","            holidays_TS = TimeSeries.from_dataframe(list_of_holidays_per_store[i], \n","                                        time_col = 'date',\n","                                        fill_missing_dates=True,\n","                                        fillna_value=0,\n","                                        freq='D')\n","            \n","            holidays_TS = holidays_TS.slice(pd.Timestamp('20130101'),pd.Timestamp('20170831'))\n","            holidays_TS = holidays_TS.astype(np.float32)\n","            listofseries.append(holidays_TS)\n","\n","    return listofseries\n","\n","\n","list_of_holidays_per_store = holiday_list(df_stores)\n","list_of_holidays_per_store = remove_0_and_duplicates(list_of_holidays_per_store)   \n","list_of_holidays_store = holiday_TS_list_54(list_of_holidays_per_store)\n","\n","holidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","holidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","\n","holidays_pipeline = Pipeline([holidays_filler, holidays_scaler])\n","holidays_transformed = holidays_pipeline.fit_transform(list_of_holidays_store)\n","\n","# Stack Together Store-Specific Covariates with General Covariates\n","\n","store_covariates_future = []\n","\n","for store in range(0,len(store_list)):\n","  stacked_covariates = holidays_transformed[store].stack(general_covariates)  \n","  store_covariates_future.append(stacked_covariates)\n","\n","store_covariates_past = []\n","holidays_transformed_sliced = holidays_transformed # for slicing past covariates\n","\n","for store in range(0,len(store_list)):\n","  holidays_transformed_sliced[store] = holidays_transformed[store].slice_intersect(transactions_covs[store])\n","  general_covariates_sliced = general_covariates.slice_intersect(transactions_covs[store])\n","  stacked_covariates = transactions_covs[store].stack(holidays_transformed_sliced[store]).stack(general_covariates_sliced)  \n","  store_covariates_past.append(stacked_covariates)\n","    \n","# Store/Family-Varying Covariates (Promotion)\n","\n","df_promotion = pd.concat([df_train, df_test], axis=0)\n","df_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\n","df_promotion.tail()\n","\n","family_promotion_dict = {}\n","\n","for family in family_list:\n","  df_family = df_promotion.loc[df_promotion['family'] == family]\n","\n","  list_of_TS_promo = TimeSeries.from_group_dataframe(\n","                                df_family,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                value_cols=\"onpromotion\", # covariate of interest\n","                                fill_missing_dates=True,\n","                                freq='D')\n","  \n","  for ts in list_of_TS_promo:\n","            ts = ts.astype(np.float32)\n","\n","  family_promotion_dict[family] = list_of_TS_promo\n","\n","promotion_transformed_dict = {}\n","\n","for key in tqdm(family_promotion_dict):\n","  promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","  promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","  promo_pipeline = Pipeline([promo_filler,\n","                             promo_scaler])\n","  \n","  promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n","\n","  # Moving Averages for Promotion Family Dictionaries\n","  promo_moving_average_7 = MovingAverage(window=7)\n","  promo_moving_average_28 = MovingAverage(window=28)\n","\n","  promotion_covs = []\n","\n","  for ts in promotion_transformed:\n","    ma_7 = promo_moving_average_7.filter(ts)\n","    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n","    ma_7 = ma_7.astype(np.float32)\n","    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n","    ma_28 = promo_moving_average_28.filter(ts)\n","    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n","    ma_28 = ma_28.astype(np.float32)\n","    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n","    promo_and_mas = ts.stack(ma_7).stack(ma_28)\n","    promotion_covs.append(promo_and_mas)\n","\n","  promotion_transformed_dict[key] = promotion_covs\n","\n","# 2.5. Assemble All Covariates in Dictionaries\n","\n","past_covariates_dict = {}\n","\n","for key in tqdm(promotion_transformed_dict):\n","\n","  promotion_family = promotion_transformed_dict[key]\n","  sales_mas = sales_moving_averages_dict[key]\n","  covariates_past = [promotion_family[i].slice_intersect(store_covariates_past[i]).stack(store_covariates_past[i].stack(sales_mas[i])) for i in range(0,len(promotion_family))]\n","\n","  past_covariates_dict[key] = covariates_past\n","\n","future_covariates_dict = {}\n","\n","for key in tqdm(promotion_transformed_dict):\n","\n","  promotion_family = promotion_transformed_dict[key]\n","  covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n","\n","  future_covariates_dict[key] = covariates_future\n","\n","only_past_covariates_dict = {}\n","\n","for key in tqdm(sales_moving_averages_dict):\n","  sales_moving_averages = sales_moving_averages_dict[key]\n","  only_past_covariates = [sales_moving_averages[i].stack(transactions_covs[i]) for i in range(0,len(sales_moving_averages))]\n","\n","  only_past_covariates_dict[key] = only_past_covariates\n","\n","# Delete Original Dataframes to Save Memory\n","\n","del(df_train)\n","del(df_test)\n","del(df_stores)\n","del(df_holidays_events)\n","del(df_oil)\n","del(df_transactions)\n","gc.collect()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"2.2.\"></a> <br>\n","# 2.2. EDA"]},{"cell_type":"markdown","metadata":{},"source":["For a first impression, let's look at a few of the 1782 (store x product family) timeseries:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-10-25T08:30:33.746539Z","iopub.status.busy":"2022-10-25T08:30:33.746012Z","iopub.status.idle":"2022-10-25T08:30:34.603037Z","shell.execute_reply":"2022-10-25T08:30:34.602081Z","shell.execute_reply.started":"2022-10-25T08:30:33.746507Z"},"trusted":true},"outputs":[],"source":["# Some EDA\n","\n","bread_series = family_TS_dict['BREAD/BAKERY'][0]\n","celebration_series = family_TS_dict['CELEBRATION'][11]\n","\n","\n","# Let's print two of the 1782 TimeSeries\n","\n","plt.subplots(2, 2, figsize=(15, 6))\n","plt.subplot(1, 2, 1) # row 1, col 2 index 1\n","bread_series.plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","celebration_series.plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Two Out Of 1782 TimeSeries\")\n","           \n","plt.subplot(1, 2, 2) # index 2\n","bread_series[-365:].plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","celebration_series[-365:].plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Only The Last 365 Days\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Let's also plot the ACF for both series and investigate the seasonal patterns:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:30:37.391919Z","iopub.status.busy":"2022-10-25T08:30:37.391568Z","iopub.status.idle":"2022-10-25T08:30:37.9299Z","shell.execute_reply":"2022-10-25T08:30:37.928947Z","shell.execute_reply.started":"2022-10-25T08:30:37.391889Z"},"trusted":true},"outputs":[],"source":["# Inspect Seasonality\n","\n","plot_acf(fill_missing_values(bread_series), m=7, alpha=0.05)\n","plt.title(\"{}, store {} in {}\".format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","plot_acf(fill_missing_values(celebration_series), alpha=0.05)\n","plt.title(\"{}, store {} in {}\".format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]));"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, the BREAD/BAKERY series displays strong weekly seasonality, as we would expect. The CELEBRATION series however has a much less clear seasonal pattern. \n","\n","I encoded the static covariates and applied 0-1 Scaling + Log-Transformation to all series. Static covariates don't vary over time - examples in our dataset are the store number or region. Scaling is important for many of the deep learning models, and the logarithmic transformation of the training data will help against undershooting the actual sales with our forecasts."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:30:40.858518Z","iopub.status.busy":"2022-10-25T08:30:40.858006Z","iopub.status.idle":"2022-10-25T08:30:48.596174Z","shell.execute_reply":"2022-10-25T08:30:48.594915Z","shell.execute_reply.started":"2022-10-25T08:30:40.858486Z"},"trusted":true},"outputs":[],"source":["# Show the Differenced Series\n","\n","# First Transform the Example Series\n","train_filler_bread = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","static_cov_transformer_bread = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") \n","log_transformer_bread = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","train_scaler_bread = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","train_filler_celebration = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","static_cov_transformer_celebration = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") \n","log_transformer_celebration = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","train_scaler_celebration = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","train_pipeline_bread = Pipeline([train_filler_bread,\n","                             static_cov_transformer_bread,\n","                             log_transformer_bread,\n","                             train_scaler_bread])\n","\n","train_pipeline_celebration = Pipeline([train_filler_celebration,\n","                             static_cov_transformer_celebration,\n","                             log_transformer_celebration,\n","                             train_scaler_celebration])\n","     \n","bread_series_transformed = train_pipeline_bread.fit_transform(bread_series)\n","celebration_series_transformed = train_pipeline_celebration.fit_transform(celebration_series)\n","\n","# Plots\n","\n","plt.subplots(2, 2, figsize=(15, 6))\n","plt.subplot(1, 2, 1) # row 1, col 2 index 1\n","bread_series_transformed.plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"TimeSeries After Scaling and Log-Transform\")\n","           \n","plt.subplot(1, 2, 2) # index 2\n","bread_series_transformed[-365:].plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Only The Last 365 Days\")\n","plt.show()\n","\n","plt.subplots(2, 2, figsize=(15, 6))\n","plt.subplot(1, 2, 1) # row 1, col 2 index 1\n","celebration_series_transformed.plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"TimeSeries After Scaling and Log-Transform\")\n","           \n","plt.subplot(1, 2, 2) # index 2\n","celebration_series_transformed[-365:].plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Only The Last 365 Days\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Covariates\n","\n","Let's look at the covariates for the last 180 days of the BREAD/BAKERY series in store 1.\n","\n","### Sales Moving Average Terms\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:14:34.165177Z","iopub.status.busy":"2022-10-25T08:14:34.164793Z","iopub.status.idle":"2022-10-25T08:14:34.550614Z","shell.execute_reply":"2022-10-25T08:14:34.549307Z","shell.execute_reply.started":"2022-10-25T08:14:34.165146Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","family_TS_transformed_dict['BREAD/BAKERY'][0][-180:].plot()\n","sales_moving_averages_dict['BREAD/BAKERY'][0][-180:].plot()\n","plt.title(\"Sales 7- and 28-day Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Promotion Data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:15:04.351471Z","iopub.status.busy":"2022-10-25T08:15:04.350799Z","iopub.status.idle":"2022-10-25T08:15:04.754443Z","shell.execute_reply":"2022-10-25T08:15:04.753522Z","shell.execute_reply.started":"2022-10-25T08:15:04.351436Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","promotion_transformed_dict['BREAD/BAKERY'][0][-180:].plot()\n","plt.title(\"Promotion Data and Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Transactions"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:15:10.690661Z","iopub.status.busy":"2022-10-25T08:15:10.690112Z","iopub.status.idle":"2022-10-25T08:15:11.168121Z","shell.execute_reply":"2022-10-25T08:15:11.167007Z","shell.execute_reply.started":"2022-10-25T08:15:10.69062Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","transactions_covs[0][-180:].plot()\n","plt.title(\"Transactions Data and Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Oil Price\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:15:24.356612Z","iopub.status.busy":"2022-10-25T08:15:24.35625Z","iopub.status.idle":"2022-10-25T08:15:24.900334Z","shell.execute_reply":"2022-10-25T08:15:24.89402Z","shell.execute_reply.started":"2022-10-25T08:15:24.356582Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","oil_transformed[-180:].plot()\n","oil_moving_averages[-180:].plot()\n","plt.title(\"Oil Price and Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Time Dummies and Covariates\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-17T01:22:45.246694Z","iopub.status.busy":"2022-11-17T01:22:45.246244Z","iopub.status.idle":"2022-11-17T01:22:45.833908Z","shell.execute_reply":"2022-11-17T01:22:45.832889Z","shell.execute_reply.started":"2022-11-17T01:22:45.246655Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","time_cov_transformed[-180:].plot()\n","plt.title(\"Time-Related Covariates\");"]},{"cell_type":"markdown","metadata":{},"source":["### Holidays and Events\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["I ordered the available data on holidays in the following seven categories. I think it might work better to generalize the categories further than what I did here."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T01:22:16.74045Z","iopub.status.busy":"2022-11-17T01:22:16.740091Z","iopub.status.idle":"2022-11-17T01:22:16.993646Z","shell.execute_reply":"2022-11-17T01:22:16.989263Z","shell.execute_reply.started":"2022-11-17T01:22:16.740421Z"},"trusted":true},"outputs":[],"source":["#df_holidays_events['type'].value_counts().plot.bar(rot=0)\n","plt.figure(figsize=(10, 6))\n","list_of_holidays_per_store[0].loc[:, list_of_holidays_per_store[0].columns != \"date\"].sum().plot.bar(rot=0)\n","plt.title(\"Holidays and Events\");"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"3\"></a> <br>\n","# 3. Simple Baseline Models"]},{"cell_type":"markdown","metadata":{},"source":["Before thinking about neural networks, I started by setting a baseline with traditional and more simple methods. The baseline forecast performance then constitutes a lower bound for the Machine Learning models' expected performance. I used three simple-to-implement models:\n","\n","* Naive Seasonal Model (repeating the last 7 days)\n","* Exponential Smoothing\n","* Facebook Prophet\n","\n","**Exponential Smoothing** gave the best results. For computational reasons, I commented out the other two models' training/evaluation later on."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"3.1.\"></a> <br>\n","# 3.1. Some Quick Backtests"]},{"cell_type":"markdown","metadata":{},"source":["Let's quickly look at **backtests (historical forecasts)** for two individual series from our large dataset. We start with the sales for **Bread and Bakery in store 1** - one of the more consistent and seasonal series in our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:31:02.255911Z","iopub.status.busy":"2022-10-25T08:31:02.255498Z","iopub.status.idle":"2022-10-25T08:31:26.818711Z","shell.execute_reply":"2022-10-25T08:31:26.817559Z","shell.execute_reply.started":"2022-10-25T08:31:02.255876Z"},"trusted":true},"outputs":[],"source":["# Define Models\n","\n","from darts.models import NaiveSeasonal, ExponentialSmoothing, Prophet\n","from darts.timeseries import concatenate\n","import logging\n","cmdstanpy_logger = logging.getLogger(\"cmdstanpy\")\n","cmdstanpy_logger.disabled = True\n","\n","Naive_Seasonal_Model = NaiveSeasonal(K=7)\n","\n","Exponential_Smoothing_Model = ExponentialSmoothing()\n","\n","Prophet_Model = Prophet()\n","\n","def eval_backtest(backtest_series, actual_series, horizon, transformer, model):\n","    actualdata = transformer.inverse_transform(actual_series, partial=True)\n","    forecasts = transformer.inverse_transform(backtest_series, partial=True)\n","    plt.figure(figsize=(10, 6))\n","    actualdata[-365:].plot(label=\"Actual Data\")\n","    forecasts.plot(label=model)\n","    plt.legend()\n","    plt.suptitle(\"{} in store {} ({})\".format(static_cov_transformer_bread.inverse_transform(actual_series).static_covariates_values()[0,1], \n","                                                static_cov_transformer_bread.inverse_transform(actual_series).static_covariates_values()[0,0],\n","                                                static_cov_transformer_bread.inverse_transform(actual_series).static_covariates_values()[0,2]))\n","    plt.title(\"Backtest with {}-months horizon, RMSLE = {:.2f}\".format(horizon,\n","            rmsle(actual_series=actualdata, pred_series=forecasts)))    \n","    \n","backtest_series_SN = Naive_Seasonal_Model.historical_forecasts(\n","    bread_series_transformed,\n","    start=pd.Timestamp('20161101'),\n","    forecast_horizon=16,\n","    stride=16,\n","    last_points_only=False,\n","    retrain=True,\n","    verbose=False,\n",")\n","\n","backtest_series_ES = Exponential_Smoothing_Model.historical_forecasts(\n","    bread_series_transformed,\n","    start=pd.Timestamp('20161101'),\n","    forecast_horizon=16,\n","    stride=16,\n","    last_points_only=False,\n","    retrain=True,\n","    verbose=False,\n",")\n","\n","backtest_series_Prophet = Prophet_Model.historical_forecasts(\n","    bread_series_transformed,\n","    start=pd.Timestamp('20161101'),\n","    forecast_horizon=16,\n","    stride=16,\n","    last_points_only=False,\n","    retrain=True,\n","    verbose=False,\n",")\n","\n","\n","eval_backtest(\n","    backtest_series=concatenate(backtest_series_SN),\n","    actual_series=bread_series_transformed,\n","    horizon=16,\n","    transformer=train_pipeline_bread,\n","    model=\"Seasonal Naive (K=7) Forecasts\"\n",")\n","\n","eval_backtest(\n","    backtest_series=concatenate(backtest_series_ES),\n","    actual_series=bread_series_transformed,\n","    horizon=16,\n","    transformer=train_pipeline_bread,\n","    model=\"Exponential Smoothing Forecasts\"\n",")\n","\n","eval_backtest(\n","    backtest_series=concatenate(backtest_series_Prophet),\n","    actual_series=bread_series_transformed,\n","    horizon=16,\n","    transformer=train_pipeline_bread,\n","    model=\"Facebook Prophet Forecasts\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Except for the zero sales around christmas/new year 2017, the series follows a **very consistent pattern** - people eat bread every week. All three models are able to predict this pattern pretty well. The Seasonal Naive model however fails heavily in the aftermath of this one observed downward spike.\n","\n","Let's now look at a **more complicated series** - as an example I picked the product family **CELEBRATION in store 19**. Though this series also has seasonal patterns, it displays much **more spikes**. How will our baseline models fare now?"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:31:53.966042Z","iopub.status.busy":"2022-10-25T08:31:53.964998Z","iopub.status.idle":"2022-10-25T08:32:16.093486Z","shell.execute_reply":"2022-10-25T08:32:16.092344Z","shell.execute_reply.started":"2022-10-25T08:31:53.965996Z"},"trusted":true},"outputs":[],"source":["def eval_backtest(backtest_series, actual_series, horizon, transformer, model):\n","    actualdata = transformer.inverse_transform(actual_series, partial=True)\n","    forecasts = transformer.inverse_transform(backtest_series, partial=True)\n","    plt.figure(figsize=(10, 6))\n","    actualdata[-365:].plot(label=\"Actual Data\")\n","    forecasts.plot(label=model)\n","    plt.legend()\n","    plt.suptitle(\"{} in store {} ({})\".format(static_cov_transformer_celebration.inverse_transform(actual_series).static_covariates_values()[0,1], \n","                                                static_cov_transformer_celebration.inverse_transform(actual_series).static_covariates_values()[0,0],\n","                                                static_cov_transformer_celebration.inverse_transform(actual_series).static_covariates_values()[0,2]))\n","    plt.title(\"Backtest with {}-months horizon, RMSLE = {:.2f}\".format(horizon,\n","            rmsle(actual_series=actualdata, pred_series=forecasts)))    \n","\n","backtest_series_SN_2 = Naive_Seasonal_Model.historical_forecasts(\n","    celebration_series_transformed,\n","    start=pd.Timestamp('20161101'),\n","    forecast_horizon=16,\n","    stride=16,\n","    last_points_only=False,\n","    retrain=True,\n","    verbose=False,\n",")\n","\n","backtest_series_ES_2 = Exponential_Smoothing_Model.historical_forecasts(\n","    celebration_series_transformed,\n","    start=pd.Timestamp('20161101'),\n","    forecast_horizon=16,\n","    stride=16,\n","    last_points_only=False,\n","    retrain=True,\n","    verbose=False,\n",")\n","\n","backtest_series_Prophet_2 = Prophet_Model.historical_forecasts(\n","    celebration_series_transformed,\n","    start=pd.Timestamp('20161101'),\n","    forecast_horizon=16,\n","    stride=16,\n","    last_points_only=False,\n","    retrain=True,\n","    verbose=False,\n",")\n","\n","\n","eval_backtest(\n","    backtest_series=concatenate(backtest_series_SN_2),\n","    actual_series=celebration_series_transformed,\n","    horizon=16,\n","    transformer=train_pipeline_celebration,\n","    model=\"Seasonal Naive (K=7) Forecasts\"\n",")\n","\n","eval_backtest(\n","    backtest_series=concatenate(backtest_series_ES_2),\n","    actual_series=celebration_series_transformed,\n","    horizon=16,\n","    transformer=train_pipeline_celebration,\n","    model=\"Exponential Smoothing Forecasts\"\n",")\n","\n","eval_backtest(\n","    backtest_series=concatenate(backtest_series_Prophet_2),\n","    actual_series=celebration_series_transformed,\n","    horizon=16,\n","    transformer=train_pipeline_celebration,\n","    model=\"Facebook Prophet Forecasts\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Those forecasts are not ideal - predicting sales is not that easy after all. The spikes in \"CELEBRATION\" products are likely caused by special events. While the Seasonal Naive model generates many falsely predicted spikes (and misses most of the actual ones), the Exponential Smoothing and Prophet models capture underlying seasonal patterns and trends, but miss all spikes. Following those forecasts, our supermarkets would not carry enough celebration merchandise when it is demanded the most, missing a lot of profit.\n","\n","From this little experiment, we can already see that good models must both capture the general seasonalities and trends in product sales, as well as understand predictable spikes and other special patterns. As we have data on holidays for example, that should hopefully be possible to some extent."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"3.2.\"></a> <br>\n","# 3.2. Training/Test Split Performance Comparison"]},{"cell_type":"markdown","metadata":{},"source":["From now on, I will use a **simple training/test split** to evaluate models on all series - the forecast horizon will be **16 days** in order to mimic the leaderboard prediction task. While rolling window validation (as in the backtests before) is a more reliable approach, it would require too much computational resources during the experimental phase of the forecasting project. Once we start training neural network models, this will become obvious."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:32:21.369096Z","iopub.status.busy":"2022-10-25T08:32:21.36869Z","iopub.status.idle":"2022-10-25T08:32:21.850017Z","shell.execute_reply":"2022-10-25T08:32:21.849023Z","shell.execute_reply.started":"2022-10-25T08:32:21.36906Z"},"trusted":true},"outputs":[],"source":["# Show example training and validation set for target series\n","\n","training_series_bread = bread_series_transformed[:-16]\n","val_series_bread = bread_series_transformed[-16:]\n","\n","plt.figure(figsize=(10, 6))\n","training_series_bread[-100:].plot(label='Training')\n","val_series_bread.plot(label='Validation')\n","plt.legend()\n","plt.title(\"{} in store {} ({})\".format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]));"]},{"cell_type":"markdown","metadata":{},"source":["For our baseline methods, we now train **1782 models, one for each (store x family) TimeSeries**. The forecasts created by these models will then be transformed back to the original scale. Additionally, we predict pure zero-forecasts for all series, which had no sales in the last two weeks (somewhat arbitrary choice)."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T08:32:40.811008Z","iopub.status.busy":"2022-10-25T08:32:40.810619Z","iopub.status.idle":"2022-10-25T08:43:00.414694Z","shell.execute_reply":"2022-10-25T08:43:00.412906Z","shell.execute_reply.started":"2022-10-25T08:32:40.810969Z"},"trusted":true},"outputs":[],"source":["# Functions for Exponential Smoothing Models and Forecasts\n","\n","def ESModelBuilder(training_list):\n","\n","    listofESmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        ES_model = ExponentialSmoothing()\n","        ES_model.fit(training_list[i])\n","        listofESmodels.append(ES_model)\n","\n","    return listofESmodels \n","\n","def ESForecaster(model_list):\n","\n","    listofESpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_ES = model_list[i].predict(n=16)\n","        listofESpreds.append(pred_ES)        \n","\n","    return listofESpreds \n","\n","# Train and Forecast with Exponential Smoothing Models\n","\n","ES_Models_Family_Dict = {}\n","ES_Forecasts_Family_Dict = {}\n","\n","# get the start time\n","st = time.time()\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  ES_Models_Family_Dict[family] = ESModelBuilder(training_data)\n","  forecasts_ES = ESForecaster(ES_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  ES_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_ES, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(ES_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-14:] == 0).all():\n","          ES_Forecasts_Family_Dict[family][i] = ES_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_exp = et - st\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-19T22:47:03.227116Z","iopub.status.busy":"2022-09-19T22:47:03.226037Z","iopub.status.idle":"2022-09-19T22:59:18.914242Z","shell.execute_reply":"2022-09-19T22:59:18.912998Z","shell.execute_reply.started":"2022-09-19T22:47:03.227037Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","# Functions for Exponential Smoothing Models and Forecasts\n","\n","def NSModelBuilder(training_list):\n","\n","    listofNSmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        NS_model = NaiveSeasonal(K=7)\n","        NS_model.fit(training_list[i])\n","        listofNSmodels.append(NS_model)\n","\n","    return listofNSmodels \n","\n","def NSForecaster(model_list):\n","\n","    listofNSpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_NS = model_list[i].predict(n=16)\n","        listofNSpreds.append(pred_NS)        \n","\n","    return listofNSpreds \n","\n","def ESModelBuilder(training_list):\n","\n","    listofESmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        ES_model = ExponentialSmoothing()\n","        ES_model.fit(training_list[i])\n","        listofESmodels.append(ES_model)\n","\n","    return listofESmodels \n","\n","def ESForecaster(model_list):\n","\n","    listofESpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_ES = model_list[i].predict(n=16)\n","        listofESpreds.append(pred_ES)        \n","\n","    return listofESpreds \n","\n","def ProphetModelBuilder(training_list):\n","\n","    listofPmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        P_model = Prophet()\n","        P_model.fit(training_list[i])\n","        listofPmodels.append(P_model)\n","\n","    return listofPmodels \n","\n","def ProphetForecaster(model_list):\n","\n","    listofPpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_P = model_list[i].predict(n=16)\n","        listofPpreds.append(pred_P)        \n","\n","    return listofPpreds \n","\n","# Train and Forecast with Exponential Smoothing Models\n","\n","NS_Models_Family_Dict = {}\n","NS_Forecasts_Family_Dict = {}\n","ES_Models_Family_Dict = {}\n","ES_Forecasts_Family_Dict = {}\n","Prophet_Models_Family_Dict = {}\n","Prophet_Forecasts_Family_Dict = {}\n","\n","import time\n","from multiprocessing import Pool\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  NS_Models_Family_Dict[family] = NSModelBuilder(training_data)\n","  forecasts_NS = NSForecaster(NS_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  NS_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_NS, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(NS_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          NS_Forecasts_Family_Dict[family][i] = NS_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","    \n","    \n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  ES_Models_Family_Dict[family] = ESModelBuilder(training_data)\n","  forecasts_ES = ESForecaster(ES_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  ES_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_ES, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(ES_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          ES_Forecasts_Family_Dict[family][i] = ES_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","    \n"," # commented out due to long computation\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  Prophet_Models_Family_Dict[family] = ProphetModelBuilder(training_data)\n","  forecasts_Prophet = ProphetForecaster(Prophet_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  Prophet_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_Prophet, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(Prophet_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          Prophet_Forecasts_Family_Dict[family][i] = Prophet_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","          \"\"\" "]},{"cell_type":"markdown","metadata":{},"source":["Let's check the RMSLE scores on the 16-days validation set we created:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:47:20.200184Z","iopub.status.busy":"2022-10-25T08:47:20.199792Z","iopub.status.idle":"2022-10-25T08:47:49.282278Z","shell.execute_reply":"2022-10-25T08:47:49.281087Z","shell.execute_reply.started":"2022-10-25T08:47:20.200151Z"},"trusted":true},"outputs":[],"source":["# Re-Format Forecasts from Dictionaries to One List\n","\n","forecast_list_ES = []\n","\n","for family in family_list:\n","  forecast_list_ES.append(ES_Forecasts_Family_Dict[family])\n","\n","sales_data = []\n","\n","for family in family_list:\n","  sales_data.append(family_TS_dict[family])\n","\n","# Function to Flatten Nested Lists\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","    \n","actual_list = flatten(sales_data)\n","pred_list_ES = flatten(forecast_list_ES)\n","\n","# Mean RMSLE\n","ES_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_ES,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","print(\"\\n\")\n","print(\"The mean RMSLE for the Local Exponential Smoothing Models over 1782 series is {:.5f}.\".format(ES_rmsle))\n","print('Training & Inference duration:', elapsed_time_exp, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-19T23:07:22.692446Z","iopub.status.busy":"2022-09-19T23:07:22.692005Z","iopub.status.idle":"2022-09-19T23:07:22.713172Z","shell.execute_reply":"2022-09-19T23:07:22.712236Z","shell.execute_reply.started":"2022-09-19T23:07:22.69241Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","\n","# Re-Format Forecasts from Dictionaries to One List\n","\n","forecast_list_NS = []\n","\n","for family in tqdm(family_list):\n","  forecast_list_NS.append(NS_Forecasts_Family_Dict[family])\n","\n","forecast_list_ES = []\n","\n","for family in tqdm(family_list):\n","  forecast_list_ES.append(ES_Forecasts_Family_Dict[family])\n","\n","#forecast_list_Prophet = []\n","\n","#for family in tqdm(family_list):\n","#  forecast_list_Prophet.append(Prophet_Forecasts_Family_Dict[family])\n","\n","sales_data = []\n","\n","for family in tqdm(family_list):\n","  sales_data.append(family_TS_dict[family])\n","\n","# Function to Flatten Nested Lists\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","    \n","actual_list = flatten(sales_data)\n","pred_list_NS = flatten(forecast_list_NS)\n","pred_list_ES = flatten(forecast_list_ES)\n","#pred_list_Prophet = flatten(forecast_list_Prophet)\n","\"\"\"\n","\n","\"\"\"\n","# Mean RMSLE\n","\n","NS_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_NS,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","ES_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_ES,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","#Prophet_rmsle = rmsle(actual_series = actual_list,\n","#                 pred_series = pred_list_Prophet,\n","#                 n_jobs = -1,\n","#                 inter_reduction=np.mean)\n","\n","print(\"The mean RMSLE for the Naive Seasonal (K=7) Model over all 1782 series is {:.5f}.\".format(NS_rmsle))\n","print(\"\\n\")\n","print(\"The mean RMSLE for Exponential Smoothing over all 1782 series is {:.5f}.\".format(ES_rmsle))\n","print(\"\\n\")\n","#print(\"The mean RMSLE for Prophet over all 1782 series is {:.5f}.\".format(Prophet_rmsle))\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["**Exponential Smoothing achieves the smallest error (RMSLE = 0.37411) for our validation data!**\n","\n","To further investigate those models' performance, let's print the **mean RMSLE for each product category**:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-10-25T08:57:26.052979Z","iopub.status.busy":"2022-10-25T08:57:26.051904Z","iopub.status.idle":"2022-10-25T08:58:02.930774Z","shell.execute_reply":"2022-10-25T08:58:02.928787Z","shell.execute_reply.started":"2022-10-25T08:57:26.052916Z"},"trusted":true},"outputs":[],"source":["# Mean RMSLE for Families\n","\n","family_forecast_rmsle_ES = {}\n","\n","for family in family_list:\n","\n","  ES_rmsle_family = rmsle(actual_series = family_TS_dict[family],\n","                 pred_series = ES_Forecasts_Family_Dict[family],\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","  \n","  family_forecast_rmsle_ES[family] = ES_rmsle_family\n","\n","\n","family_forecast_rmsle_ES = dict(sorted(family_forecast_rmsle_ES.items(), key=lambda item: item[1]))\n","\n","print(\"\\n\")\n","print(\"Mean RMSLE for the 33 different product families, from worst to best:\")\n","print(\"\\n\")\n","\n","# Iterate over key/value pairs in dict and print them\n","for key, value in family_forecast_rmsle_ES.items():\n","    print(key, ' : ', value)"]},{"cell_type":"markdown","metadata":{},"source":[" I also plot the **three worst forecasts generated** - maybe we can learn something:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-10-25T09:34:37.191009Z","iopub.status.busy":"2022-10-25T09:34:37.189752Z","iopub.status.idle":"2022-10-25T09:35:07.783836Z","shell.execute_reply":"2022-10-25T09:35:07.78292Z","shell.execute_reply.started":"2022-10-25T09:34:37.190945Z"},"trusted":true},"outputs":[],"source":["# Plot the five worst forecasts   #family_TS_dict\n","\n","errorlist = []\n","\n","for i in range(0, len(actual_list)):\n","\n","  error = rmsle(actual_series = actual_list[i], \n","                pred_series = pred_list_ES[i])\n","  \n","  errorfam = actual_list[i].static_covariates_values()[0,1]\n","  \n","  errorlist.append([errorfam,error])\n","\n","rmsle_series_ES = pd.DataFrame(errorlist,columns=['family','RMSLE'])\n","worst_3_ES = rmsle_series_ES.sort_values(by=['RMSLE'], ascending=False).head(3)\n","\n","fig,axs = plt.subplots(1,len(worst_3_ES),figsize=(20, 5))\n","labels = [\"actual data\", \"ES forecast\"]\n","for i in range(0, len(worst_3_ES)):\n","  plt_forecast = pred_list_ES[(worst_3_ES.index[i])]\n","  plt_actual = actual_list[(worst_3_ES.index[i])]\n","  plt_err = rmsle(plt_actual, plt_forecast)\n","  axis = axs[i]\n","  plt_actual[-100:].plot(ax=axis, label=\"actual data\") #, label=\"actual data\"\n","  plt_forecast.plot(ax=axis, label=\"ES forecast\") #, label=\"ES forecast\"\n","  axis.legend(loc=\"upper left\")\n","  axis.title.set_text(\"{} in store {} ({}) \\n RMSLE: {}\".format(plt_forecast.static_covariates_values()[0,1], \n","                                               plt_forecast.static_covariates_values()[0,0],\n","                                               plt_forecast.static_covariates_values()[0,2],\n","                                               plt_err))  "]},{"cell_type":"markdown","metadata":{},"source":["Apparently school has finally started;) Those forecasts are pretty bad, but the Exponential Smoothing models could not have been expected to predict a sudden peak in sales like this. My hope would be to capture such (in this case probably month/week-specific) patterns with more informed models using covariates, such as NN models."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4\"></a> <br>\n","# 4. Global Modeling"]},{"cell_type":"markdown","metadata":{},"source":["Now that we set a solid baseline, can we improve our forecasts? My current understanding of timeseries forecasting is this:\n","\n","When dealing with small datasets and few dimensions, fancy/complex models (I'm looking at you, neural networks) won't give much benefit, if any. I often found simpler statistical models outperforming them.\n","\n","Intuitively, that makes sense: Neural networks are highly complex, non-linear models that don't force any structure on the data at hand. Statistical methods like ARIMA, Exponential Smoothing or Prophet on the other hand involve a lot of fixed structure and don't offer the same amount of flexibility as neural networks or boosted tree models. But when modeling a single univariate timeseries of a couple hundred data points, we don't need all this flexbility - there is not enough signal in our data to model highly complex relationships. When it comes to capturing basic patterns like seasonality and trend, statistical methods do a great job.\n","\n","The store sales data at hand however consists of 1782 timeseries of considerable length, including a set of relevant covariates (such as holidays and product promotions). That means there should be some signal to exploit with Machine Learning models. I would expect (most of) those 1782 series to be somewhat similar/related to each other, as they all concern store sales, which should follow common patterns to some extent. So what now?\n","\n","**Global Models!** My plan is to leverage the power of NN and Boosting models to exploit/capture as much of the signal in our huge dataset as possible. Whereas local models (like our Exponential Smoothing baseline) are trained on one timeseries, global models are trained on multiple series."]},{"cell_type":"markdown","metadata":{},"source":["We will now look at three different deep learning models:\n","\n","* LSTM (Long Short-Term Memory)\n","* N-HiTS (Neural Hierarchical Interpolation for TS Forecasting)\n","* TFT (Temporal Fusion Transformer)\n","\n","The LSTM model (1995) has been around for some time, whereas TFT (2019) and N-HiTS (2022) are relatively new models. I chose these three neural network models for the different way the utilize covariates.\n","\n","LSTM is a recurrent neural network (RNN) and expects covariates that extend into the future until the forecast horizon. Within the Darts framework, those are called *future_covariates*. N-HiTS is similar to the N-BEATS model, but might offer computational benefits over it. This model can only work take in *past_covariates*, known until the point in time when a forecast is generated. Last but not least, the TFT model supports both *past_covariates* and *future_covariates* as well as *static_covariates*."]},{"cell_type":"markdown","metadata":{},"source":["Before training those models, I performed some hyperparameter tuning with the help of Python's Optuna library. As this can take quite some time for NN models, I let the tuning run with a quicker GPU on Google Colab. TFT seems to be the computationally heaviest out of those three NN models, while N-HiTS runs the quickest. For that reason, I will only include the Tuning for this model in my notebook.\n","\n","### COMPUTATION ISSUES\n","\n","Importantly, the following models are all trained with small subsets of the full timeseries (see the parameter *max_samples_per_ts* in the code) in order to make computation feasible. The Kaggle GPU is not quick enough to deal with bigger data I think. Therefore the following models should not be regarded as optimal at all, but rather serve as minimal examples.\n","\n","CatBoost is trained with the last 365 samples (input + output lengths) of each series, N-HiTS is trained with the last 180 samples, LSTM is trained with the last 60 samples, and TFT is trained with only the last 7 samples per series. I chose those numbers to roughly equalize the training time for each of those models after experimenting a bit."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.1\"></a> <br>\n","# 4.1. N-HiTS"]},{"cell_type":"markdown","metadata":{},"source":["\n","<img style=\"float: center;\" src=\"https://images.deepai.org/converted-papers/2201.12886/x4.png\">\n","\n","*source*: https://images.deepai.org/converted-papers/2201.12886/x4.png"]},{"cell_type":"markdown","metadata":{},"source":["N-HiTS only supports *past_covariates*. As I still want to use the future-known information on promotion, holidays and time dummies, I shift back those covariates 16 days to the past. I define a function for training the model and then create an Optuna study, which I let run for 5 trials on the relatively slow Kaggle GPU. That is not a lot and will likely not deliver very good hyperparameters, but suffices as an example."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T09:50:34.868716Z","iopub.status.busy":"2022-10-25T09:50:34.868101Z","iopub.status.idle":"2022-10-25T09:51:18.318021Z","shell.execute_reply":"2022-10-25T09:51:18.316865Z","shell.execute_reply.started":"2022-10-25T09:50:34.868672Z"},"trusted":true},"outputs":[],"source":["# Data Preparation for N-HiTS\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","# Shift future covariates back so they can be used as past covariates\n","\n","only_past_covariates = []\n","\n","for family in family_list:\n","  only_past_covariates.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates = flatten(only_past_covariates)\n","\n","NHiTS_covariates = []\n","\n","for i in range(0,len(future_covariates_full)):\n","  shifted = future_covariates_full[i].shift(n=-16)\n","  cut = shifted.slice_intersect(only_past_covariates[i])\n","  stacked = cut.stack(only_past_covariates[i])\n","  NHiTS_covariates.append(stacked)\n","    \n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","train = [s[: -(2 * val_len)] for s in training_transformed]\n","val = [s[-(2 * val_len) : -val_len] for s in training_transformed]\n","test = [s[-val_len:] for s in training_transformed]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T10:04:37.538065Z","iopub.status.busy":"2022-10-25T10:04:37.537703Z","iopub.status.idle":"2022-10-25T10:04:37.552442Z","shell.execute_reply":"2022-10-25T10:04:37.551403Z","shell.execute_reply.started":"2022-10-25T10:04:37.538032Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a N-HiTS Model, which we will re-use later.\n","\"\"\"\n","\n","def build_fit_nhits_model(\n","    input_chunk_length,\n","    num_stacks,\n","    num_blocks,\n","    num_layers,\n","    layer_exp,\n","    dropout,\n","    lr,\n","    likelihood=None,\n","    callbacks=None,\n","    #max_samples=None\n","):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_N_EPOCHS = 50\n","    MAX_SAMPLES_PER_TS = 180\n","\n","    # throughout training we'll monitor the validation loss for early stopping\n","    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0001, patience=2, verbose=True)\n","    if callbacks is None:\n","        callbacks = [early_stopper]\n","    else:\n","        callbacks = [early_stopper] + callbacks\n","\n","    # detect if a GPU is available\n","    if torch.cuda.is_available():\n","        pl_trainer_kwargs = {\n","            \"accelerator\": \"gpu\",\n","            \"gpus\": 1,\n","            \"auto_select_gpus\": True,\n","            \"callbacks\": callbacks,\n","        }\n","    \n","        num_workers = 2\n","    else:\n","        pl_trainer_kwargs = {\"callbacks\": callbacks}\n","        num_workers = 0\n","\n","    # build the N-HiTS model\n","    model = NHiTSModel(\n","        input_chunk_length=input_chunk_length,\n","        output_chunk_length=16,\n","        num_stacks=num_stacks,\n","        num_blocks=num_blocks,\n","        num_layers=num_layers,\n","        layer_widths=2 ** layer_exp,\n","        dropout=dropout,\n","        n_epochs=MAX_N_EPOCHS,\n","        batch_size=128,\n","        add_encoders=None,\n","        likelihood=None, \n","        loss_fn=torch.nn.MSELoss(),\n","        random_state=42,\n","        optimizer_kwargs={\"lr\": lr},\n","        pl_trainer_kwargs=pl_trainer_kwargs,\n","        model_name=\"nhits_model\",\n","        force_reset=True,\n","        save_checkpoints=True,\n","    )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + input_chunk_length) : -val_len] for s in training_transformed]\n","\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=NHiTS_covariates,\n","        val_past_covariates=NHiTS_covariates,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n","        num_loader_workers=num_workers,\n","    )\n","\n","    # reload best model over course of training\n","    model = NHiTSModel.load_from_checkpoint(\"nhits_model\")\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T10:04:45.32871Z","iopub.status.busy":"2022-10-25T10:04:45.328165Z","iopub.status.idle":"2022-10-25T10:07:58.219704Z","shell.execute_reply":"2022-10-25T10:07:58.216675Z","shell.execute_reply.started":"2022-10-25T10:04:45.328662Z"},"trusted":true},"outputs":[],"source":["# Hyperparameter Tuning with Optuna\n","\n","def objective(trial):\n","    callback = [PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")]\n","\n","    # set input_chunk_length, between 21 and 365 days\n","    input_chunk_length = trial.suggest_int(\"input_chunk_length\", 63, 270)\n","\n","    # Other hyperparameters\n","    num_stacks = trial.suggest_int(\"num_stacks\", 1, 3)\n","    num_blocks = trial.suggest_int(\"num_blocks\", 1, 3)\n","    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n","    layer_exp = trial.suggest_int(\"layer_exp\", 7, 10)\n","    #layer_widths = 2 ** layer_exp\n","    dropout = trial.suggest_float(\"dropout\", 0.01, 0.2, step=0.01)\n","    lr = trial.suggest_float(\"lr\", 5e-5, 0.1, log=True)\n","\n","    # build and train the N-HiTS model with these hyper-parameters:\n","    model = build_fit_nhits_model(\n","                        input_chunk_length=input_chunk_length,\n","                        num_stacks=num_stacks,\n","                          num_blocks=num_blocks,\n","                        num_layers=num_layers,\n","                        layer_exp=layer_exp,\n","                          dropout=dropout,\n","                              lr=lr,\n","                          likelihood=None,\n","                          callbacks=callback,\n","                          #max_samples=365\n","    )\n","\n","    # Evaluate how good it is on the validation set\n","    preds = model.predict(series=train, past_covariates=NHiTS_covariates, n=val_len)\n","    rmsles = rmsle(val, preds, n_jobs=-1, verbose=True)\n","    rmsle_val = np.mean(rmsles)\n","\n","    return rmsle_val if rmsle_val != np.nan else float(\"inf\")\n","\n","def print_callback(study, trial):\n","    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n","    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n","\n","\n","torch.cuda.empty_cache()\n","\n","study_nhits = optuna.create_study(direction=\"minimize\")\n","\n","study_nhits.optimize(objective, n_trials=5, callbacks=[print_callback])\n","\n","# Finally, print the best value and best hyperparameters:\n","print(f\"Best value: {study_nhits.best_value}, Best params: {study_nhits.best_trial.params}\")"]},{"cell_type":"markdown","metadata":{},"source":["So this are the best hyperparameters found:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["# Finally, print the best value and best hyperparameters:\n","print(f\"Best value: {study_nhits.best_value}, Best params: {study_nhits.best_trial.params}\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's take a look at the Tuning process:"]},{"cell_type":"markdown","metadata":{},"source":["### Improvement of error score over Tuning trials"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["plot_optimization_history(study_nhits)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter Importance over Tuning trials"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["plot_param_importances(study_nhits)"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter Search space and corresponding error scores"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["plot_contour(study_nhits, params=[\"lr\", \"num_stacks\"])"]},{"cell_type":"markdown","metadata":{},"source":["Using the hyperparameters we got during this short Tuning session, we now train the N-HiTS model and make forecasts for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["nhits_params = study_nhits.best_trial.params\n","\n","# get the start time\n","st = time.time()\n","\n","NHiTS_Model = build_fit_nhits_model(**nhits_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in training_transformed] \n","preds = NHiTS_Model.predict(series=training_data, past_covariates=NHiTS_covariates, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (list_of_TS[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","        \n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_nhits = et - st\n","\n","# Mean RMSLE\n","\n","NHiTS_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"The mean RMSLE for the Global N-HiTS Model over 1782 series is {:.5f}.\".format(NHiTS_rmsle))\n","print('Training & Inference duration:', elapsed_time_nhits, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.1\"></a> <br>\n","# 4.2. LSTM"]},{"cell_type":"markdown","metadata":{},"source":["<img style=\"float: center;\" src=\"https://miro.medium.com/max/674/1*jikKbzFXCq-IYnFZankIMg.png\">\n","\n","*source*: https://miro.medium.com/max/674/1*jikKbzFXCq-IYnFZankIMg.png"]},{"cell_type":"markdown","metadata":{},"source":["LSTM only supports future-known covariates. In order to also utilize the data from our solely past-known covariates (sales and transactions data), I shift them forward 16 days into the future. While this approach might not be optimal, I found it to perform better than throwing out those covariates all-together.\n","\n","A little Fine-Tuning gave me the following hyperparameters for LSTM:\n","\n","* Input_chunk_length: 131 \n","* hidden_dim: 39 \n","* n_rnn_layers: 3\n","* learning rate: 0.0019971227090605087\n","\n","Let's train this model with all our data and make a forecast for the 16 days test set!"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-26T12:44:03.495898Z","iopub.status.busy":"2022-10-26T12:44:03.495428Z","iopub.status.idle":"2022-10-26T12:44:52.645094Z","shell.execute_reply":"2022-10-26T12:44:52.644079Z","shell.execute_reply.started":"2022-10-26T12:44:03.495838Z"},"trusted":true},"outputs":[],"source":["# Data Preparation for LSTM\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","# Shift past covariates forward so they can be used as future covariates\n","\n","\n","only_past_covariates = []\n","\n","for family in family_list:\n","  only_past_covariates.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates = flatten(only_past_covariates)\n","\n","\n","LSTM_covariates = []\n","\n","for i in range(0,len(only_past_covariates)):\n","  shifted = only_past_covariates[i].shift(n=16)\n","  cut = future_covariates_full[i].slice_intersect(shifted)\n","  stacked = cut.stack(shifted)\n","  LSTM_covariates.append(stacked)\n","    \n","# Slice-Intersect target and covariates after shifting\n","\n","LSTM_target = []\n","\n","for i in range(0, len(training_transformed)):\n","  sliced = training_transformed[i].slice_intersect(LSTM_covariates[i])\n","  LSTM_target.append(sliced)\n","\n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","LSTM_train = [s[: -(2 * val_len)] for s in LSTM_target]\n","LSTM_val = [s[-(2 * val_len) : -val_len] for s in LSTM_target]\n","LSTM_test = [s[-val_len:] for s in LSTM_target]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-26T12:45:00.49453Z","iopub.status.busy":"2022-10-26T12:45:00.494054Z","iopub.status.idle":"2022-10-26T12:45:00.507161Z","shell.execute_reply":"2022-10-26T12:45:00.505932Z","shell.execute_reply.started":"2022-10-26T12:45:00.49449Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a TCN Model, which we will re-use later.\n","\"\"\"\n","\n","def build_fit_lstm_model(\n","    input_chunk_length,\n","    hidden_dim,\n","    n_rnn_layers,\n","  #  dropout,\n","  #  training_length,\n","  #  batch_size,\n","  #  n_epochs,\n","    lr,\n","    likelihood=None,\n","    callbacks=None,\n","):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_N_EPOCHS = 100\n","    MAX_SAMPLES_PER_TS = 60\n","\n","    # throughout training we'll monitor the validation loss for early stopping\n","    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0001, patience=2, verbose=True)\n","    if callbacks is None:\n","        callbacks = [early_stopper]\n","    else:\n","        callbacks = [early_stopper] + callbacks\n","\n","    # detect if a GPU is available\n","    if torch.cuda.is_available():\n","        pl_trainer_kwargs = {\n","            \"accelerator\": \"gpu\",\n","            \"gpus\": 1,\n","            \"auto_select_gpus\": True,\n","            \"callbacks\": callbacks,\n","        }\n","        num_workers = 2\n","    else:\n","        pl_trainer_kwargs = {\"callbacks\": callbacks}\n","        num_workers = 0\n","\n","    # build the LSTM model\n","    model = RNNModel(\n","        model=\"LSTM\",\n","        input_chunk_length=input_chunk_length,\n","        hidden_dim=hidden_dim,\n","        n_rnn_layers=n_rnn_layers,\n","        dropout=0,\n","        training_length=input_chunk_length + val_len -1,\n","        n_epochs=MAX_N_EPOCHS,\n","        batch_size=128,\n","        add_encoders=None,\n","        likelihood=None, \n","        loss_fn=torch.nn.MSELoss(),\n","        random_state=42,\n","        optimizer_kwargs={\"lr\": lr},\n","        pl_trainer_kwargs=pl_trainer_kwargs,\n","        model_name=\"lstm_model\",\n","        force_reset=True,\n","        save_checkpoints=True,\n","    )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + input_chunk_length) : -val_len] for s in LSTM_target]\n","\n","\n","    # train the model\n","    model.fit(\n","        series=LSTM_train,\n","        val_series=model_val_set,\n","        future_covariates=LSTM_covariates,\n","        val_future_covariates=LSTM_covariates,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n","        num_loader_workers=num_workers,\n","    )\n","\n","    # reload best model over course of training\n","    model = RNNModel.load_from_checkpoint(\"lstm_model\")\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-26T12:45:05.155643Z","iopub.status.busy":"2022-10-26T12:45:05.154913Z","iopub.status.idle":"2022-10-26T12:46:00.732177Z","shell.execute_reply":"2022-10-26T12:46:00.722068Z","shell.execute_reply.started":"2022-10-26T12:45:05.155602Z"},"trusted":true},"outputs":[],"source":["lstm_params = {'input_chunk_length': 131, \n","               'hidden_dim': 39, \n","               'n_rnn_layers': 3, \n","               'lr': 0.0019971227090605087}\n","\n","torch.cuda.empty_cache()\n","\n","# get the start time\n","st = time.time()\n","\n","LSTM_Model = build_fit_lstm_model(**lstm_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in LSTM_target] \n","preds = LSTM_Model.predict(series=training_data, future_covariates=LSTM_covariates, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (LSTM_target[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","        \n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_lstm = et - st\n","\n","# Mean RMSLE\n","\n","LSTM_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"The mean RMSLE for the Global LSTM Model over 1782 series is {:.5f}.\".format(LSTM_rmsle))\n","print('Training & Inference duration:', elapsed_time_lstm, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.3\"></a> <br>\n","# 4.3. TFT"]},{"cell_type":"markdown","metadata":{},"source":["<img style=\"float: center;\" src=\"https://miro.medium.com/max/1400/1*7rXe_MVn5QI9oLP2vrMdvQ.png\">\n","\n","*source*: https://miro.medium.com/max/1400/1*7rXe_MVn5QI9oLP2vrMdvQ.png"]},{"cell_type":"markdown","metadata":{},"source":["The TFT model natively supports all types of covariates including static ones. It is however computationally very heavy.\n","\n","I will use those hyperparameters:\n","\n","* input_chunk_length: 230\n","* output_chunk_length: 16 \n","* hidden_size: 16 \n","* lstm_layers: 3 \n","* num_attention_heads: 4\n","* full_attention: True\n","* hidden_continuous_size: 16\n","* dropout: 0.060000000000000005\n","* lr: 0.009912733600616069\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["# Data Preparation for TFT\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","\n","only_past_covariates_full = []\n","\n","for family in family_list:\n","  only_past_covariates_full.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates_full = flatten(only_past_covariates_full)\n","\n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","train = [s[: -(2 * val_len)] for s in training_transformed]\n","val = [s[-(2 * val_len) : -val_len] for s in training_transformed]\n","test = [s[-val_len:] for s in training_transformed]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a TCN Model, which we will re-use later.\n","\"\"\"\n","\n","\n","def build_fit_tft_model(\n","    input_chunk_length,\n","    output_chunk_length,\n","    hidden_size,\n","    lstm_layers,\n","    num_attention_heads,\n","    full_attention,\n","    #feed_forward,\n","    hidden_continuous_size,\n","    dropout,\n","    #batch_size,\n","    #add_relative_index,\n","    lr,\n","    likelihood=None,\n","    callbacks=None,\n","):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_N_EPOCHS = 100\n","    MAX_SAMPLES_PER_TS = 7\n","\n","    # throughout training we'll monitor the validation loss for early stopping\n","    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0001, patience=2, verbose=True)\n","    if callbacks is None:\n","        callbacks = [early_stopper]\n","    else:\n","        callbacks = [early_stopper] + callbacks\n","\n","    # detect if a GPU is available\n","    if torch.cuda.is_available():\n","        pl_trainer_kwargs = {\n","            \"accelerator\": \"gpu\",\n","            \"gpus\": 1,\n","            \"auto_select_gpus\": True,\n","            \"callbacks\": callbacks,\n","        }\n","        num_workers = 2\n","    else:\n","        pl_trainer_kwargs = {\"callbacks\": callbacks}\n","        num_workers = 0\n","\n","    # build the TFT model\n","    model = TFTModel(\n","        input_chunk_length=input_chunk_length,\n","        output_chunk_length=output_chunk_length,\n","        hidden_size=hidden_size,\n","        lstm_layers=lstm_layers,\n","        num_attention_heads=num_attention_heads,\n","        full_attention =full_attention,\n","        #feed_forward = feed_forward,\n","        hidden_continuous_size = hidden_continuous_size,\n","        dropout=dropout,\n","        batch_size=128,\n","        n_epochs=MAX_N_EPOCHS,\n","        #add_relative_index=add_relative_index,\n","        add_encoders=None,\n","        likelihood=None, \n","        loss_fn=torch.nn.MSELoss(),\n","        random_state=42,\n","        optimizer_kwargs={\"lr\": lr},\n","        pl_trainer_kwargs=pl_trainer_kwargs,\n","        model_name=\"tft_model\",\n","        force_reset=True,\n","        save_checkpoints=True,\n","    )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + input_chunk_length) : -val_len] for s in training_transformed]\n","\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_full,\n","        val_past_covariates=only_past_covariates_full,\n","        future_covariates=future_covariates_full,\n","        val_future_covariates=future_covariates_full,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n","        num_loader_workers=num_workers,\n","    )\n","\n","    # reload best model over course of training\n","    model = TFTModel.load_from_checkpoint(\"tft_model\")\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["tft_params = {'input_chunk_length': 230, \n","              'output_chunk_length': 16, \n","              'hidden_size': 16, \n","              'lstm_layers': 3, \n","              'num_attention_heads': 4, \n","              'full_attention': True, \n","              'hidden_continuous_size': 16, \n","              'dropout': 0.060000000000000005, \n","              'lr': 0.009912733600616069}\n","\n","torch.cuda.empty_cache()\n","\n","# get the start time\n","st = time.time()\n","\n","TFT_Model = build_fit_tft_model(**tft_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in training_transformed] \n","preds = TFT_Model.predict(series=training_data, past_covariates=only_past_covariates_full, future_covariates=future_covariates_full, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (list_of_TS[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_tft = et - st\n","\n","# Mean RMSLE\n","\n","TFT_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"The mean RMSLE for the Global TFT Model over 1782 series is {:.5f}.\".format(TFT_rmsle))\n","print('Training & Inference duration:', elapsed_time_tft, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.4\"></a> <br>\n","# 4.4. Boosted Trees"]},{"cell_type":"markdown","metadata":{},"source":["<img style=\"float: center;\" src=\"https://avatars.mds.yandex.net/get-yablogs/47421/file_1548410151831/orig\">\n","\n","*source*: https://avatars.mds.yandex.net/get-yablogs/47421/file_1548410151831/orig"]},{"cell_type":"markdown","metadata":{},"source":["Even though my focus has been on deep learning models, I actually found boosted tree models to perform best for this forecasting problem so far. Darts offers implementations of LightGBM and CatBoost. \n","\n","Due to out-of-memory issues in the Kaggle kernel, I commented out the training of boosted trees for now. From former notebooks and training on Colab however I can tell that those models have given the best scores so far."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true,"execution":{"iopub.execute_input":"2022-10-29T15:21:42.562685Z","iopub.status.busy":"2022-10-29T15:21:42.560299Z","iopub.status.idle":"2022-10-29T15:21:42.61505Z","shell.execute_reply":"2022-10-29T15:21:42.612495Z","shell.execute_reply.started":"2022-10-29T15:21:42.562495Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["\"\"\"\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","\n","only_past_covariates_full = []\n","\n","for family in family_list:\n","  only_past_covariates_full.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates_full = flatten(only_past_covariates_full)\n","\n","\n","only_past_covariates_shifted = []\n","\n","for ts in only_past_covariates_full:\n","  shifted = ts.shift(n=16)\n","  only_past_covariates_shifted.append(shifted)\n","\n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","train = [s[: -(2 * val_len)] for s in training_transformed]\n","val = [s[-(2 * val_len) : -val_len] for s in training_transformed]\n","test = [s[-val_len:] for s in training_transformed]\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-29T14:58:47.443525Z","iopub.status.busy":"2022-10-29T14:58:47.443136Z","iopub.status.idle":"2022-10-29T14:58:47.45601Z","shell.execute_reply":"2022-10-29T14:58:47.454295Z","shell.execute_reply.started":"2022-10-29T14:58:47.443498Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a TCN Model, which we will re-use later.\n","\n","from darts.models import CatBoostModel\n","\n","def build_fit_cboost_model(\n","    lags,\n","    firstlag,\n","    pastcovlag,\n","    out_len,\n","    learning_rate,\n","    depth\n","    ):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_SAMPLES_PER_TS = 365\n","\n","    # build the TCN model\n","    model = CatBoostModel(lags = lags,\n","                             lags_future_covariates = (firstlag,1),\n","                             lags_past_covariates = [-pastcovlag], \n","                             output_chunk_length=out_len,\n","                          learning_rate=learning_rate,\n","                          depth=depth,\n","                             early_stopping_rounds=10,\n","                             random_state=2022,\n","                          logging_level='Silent'\n","                          )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + lags) : -val_len] for s in training_transformed]\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_shifted,\n","        val_past_covariates=only_past_covariates_shifted,\n","        future_covariates=future_covariates_full,\n","        val_future_covariates=future_covariates_full,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS\n","    )\n","\n","    # reload best model over course of training\n","    #model = LightGBMModel.load_from_checkpoint(\"lgbm_model\")\n","\n","    return model\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-29T14:58:53.851026Z","iopub.status.busy":"2022-10-29T14:58:53.8506Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","catboost_params = {'lags': 144, \n","                   'out_len': 3, \n","                   'firstlag': 44, \n","                   'pastcovlag': 60, \n","                   'learning_rate': 0.06539829509538796, \n","                   'depth': 9}\n","\n","# get the start time\n","st = time.time()\n","\n","CatBoost_Model = build_fit_cboost_model(**catboost_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in training_transformed] \n","preds = CatBoost_Model.predict(series=training_data, past_covariates=only_past_covariates_shifted, future_covariates=future_covariates_full, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (list_of_TS[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_cboost = et - st\n","\n","# Mean RMSLE\n","\n","CatBoost_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","print(\"\\n\")\n","print(\"The mean RMSLE for the Global CatBoost Model over 1782 series is {:.5f}.\".format(CatBoost_rmsle))\n","print('Training & Inference duration:', elapsed_time_cboost, 'seconds')\n","print(\"\\n\")\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-24T03:37:30.014346Z","iopub.status.busy":"2022-10-24T03:37:30.013185Z","iopub.status.idle":"2022-10-24T03:38:45.808688Z","shell.execute_reply":"2022-10-24T03:38:45.806979Z","shell.execute_reply.started":"2022-10-24T03:37:30.014307Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a CatBoost Model, which we will re-use later.\n","\"\"\"\n","\"\"\"\n","from darts.models import CatBoostModel\n","\n","def build_fit_family_cboost_model(\n","    lags,\n","    firstlag,\n","    pastcovlag,\n","    out_len,\n","    ):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # build the CatBoost model\n","    model = CatBoostModel(lags = lags,\n","                             lags_future_covariates = (firstlag,1),\n","                             lags_past_covariates = [-pastcovlag], \n","                             output_chunk_length=out_len,\n","                             early_stopping_rounds=10,\n","                             random_state=2022\n","                          )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + lags) : -val_len] for s in sales_family]\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_fam_shifted,\n","        val_past_covariates=only_past_covariates_fam_shifted,\n","        future_covariates=future_covariates_fam,\n","        val_future_covariates=future_covariates_fam\n","    )\n","\n","    return model\n","\n","# Train CatBoost Family Models\n","\n","CatBoost_Models = {}\n","\n","for family in tqdm(family_list):\n","    \n","  sales_family = family_TS_transformed_dict[family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","\n","  # Split in train/val/test\n","  val_len = 16\n","  train = [s[: -(2 * val_len)] for s in sales_family]\n","\n","  CatBoost_Model = build_fit_family_cboost_model( lags = 365,\n","    firstlag = 28,\n","    pastcovlag = 14,\n","    out_len = 1)\n","\n","  CatBoost_Models[family] = CatBoost_Model\n","  \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\"\n","# Generate Forecasts for the Test Data\n","\n","CatBoost_Forecasts_Families = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","\n","  forecast_CatBoost = CatBoost_Models[family].predict(n=16,\n","                                         series=training_data,\n","                                         future_covariates=future_covariates_fam,\n","                                         past_covariates=only_past_covariates_fam_shifted\n","                                         )\n","  \n","  CatBoost_Forecasts_Families[family] = forecast_CatBoost\n","\n","# Transform Back\n","\n","CatBoost_Forecasts_Families_back = {}\n","\n","for family in tqdm(family_list):\n","\n","  CatBoost_Forecasts_Families_back[family] = family_pipeline_dict[family].inverse_transform(CatBoost_Forecasts_Families[family], partial=True)\n","\n","# Zero Forecasting\n","\n","for family in tqdm(CatBoost_Forecasts_Families_back):\n","  for n in range(0,len(CatBoost_Forecasts_Families_back[family])):\n","    if (family_TS_dict[family][n][:-16].univariate_values()[-14:] == 0).all():\n","        CatBoost_Forecasts_Families_back[family][n] = CatBoost_Forecasts_Families_back[family][n].map(lambda x: x * 0)\n","        \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\"\n","# Re-Format all 1782 Forecasts in one List and Evaluate Performance\n","\n","forecast_list_CatBoost = []\n","\n","for family in family_list:\n","  forecast_list_CatBoost.append(CatBoost_Forecasts_Families_back[family])\n","\n","sales_data = []\n","\n","for family in family_list:\n","  sales_data.append(family_TS_dict[family])\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","    \n","actual_list = flatten(sales_data)\n","pred_list_CatBoost = flatten(forecast_list_CatBoost)\n","\n","# Mean RMSLE\n","\n","CatBoost_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_CatBoost,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","print(\"\\n\")\n","print(\"The mean RMSLE for the 33 CatBoost Global Product Family Models over all 1782 series is {:.5f}.\".format(CatBoost_rmsle))\n","print(\"\\n\")\n","\n","# Mean RMSLE for Families\n","\n","family_forecast_rmsle_CatBoost = {}\n","\n","for family in family_list:\n","\n","  CatBoost_rmsle_family = rmsle(actual_series = family_TS_dict[family],\n","                 pred_series = CatBoost_Forecasts_Families_back[family],\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","  \n","  family_forecast_rmsle_CatBoost[family] = CatBoost_rmsle_family\n","\n","family_forecast_rmsle_CatBoost = dict(sorted(family_forecast_rmsle_CatBoost.items(), key=lambda item: item[1]))\n","\n","print(\"Mean RMSLE for the 33 different product families, from worst to best:\")\n","print(\"\\n\")\n","\n","# Iterate over key/value pairs in dict and print them\n","for key, value in family_forecast_rmsle_CatBoost.items():\n","    print(key, ' : ', value)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-20T22:26:00.336799Z","iopub.status.busy":"2022-09-20T22:26:00.336282Z","iopub.status.idle":"2022-09-20T22:26:39.414778Z","shell.execute_reply":"2022-09-20T22:26:39.413574Z","shell.execute_reply.started":"2022-09-20T22:26:00.336761Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","# Plot the five worst forecasts   \n","\n","errorlist = []\n","\n","for i in range(0, len(actual_list)):\n","\n","  error = rmsle(actual_series = actual_list[i], \n","                pred_series = pred_list_CatBoost[i])\n","  \n","  errorfam = actual_list[i].static_covariates_values()[0,1]\n","  \n","  errorlist.append([errorfam,error])\n","\n","rmsle_series_CatBoost = pd.DataFrame(errorlist,columns=['family','RMSLE'])\n","worst_3_CatBoost = rmsle_series_CatBoost.sort_values(by=['RMSLE'], ascending=False).head(3)\n","\n","for i in range(0, len(worst_3_CatBoost)):\n","  plt_forecast = pred_list_CatBoost[(worst_3_CatBoost.index[i])]\n","  plt_actual = actual_list[(worst_3_CatBoost.index[i])]\n","  plt_err = rmsle(plt_actual, plt_forecast)\n","\n","  plt.figure(figsize=(10, 6))\n","  plt_actual[-100:].plot(label=\"actual data\")\n","  plt_forecast.plot(label=\"CatBoost forecast\")\n","  plt.title(\"{} in store {} ({}) - RMSLE: {}\".format(plt_forecast.static_covariates_values()[0,1], \n","                                                plt_forecast.static_covariates_values()[0,0],\n","                                                plt_forecast.static_covariates_values()[0,2],\n","                                                plt_err))\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.5\"></a> <br>\n","# 4.5. Model Comparison"]},{"cell_type":"markdown","metadata":{},"source":["Let's quickly compare the performance of the models we trained in this notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"Mean RMSLE for Local Exponential Smoothing Models: {:.5f}.\".format(ES_rmsle))\n","print('Training duration:', elapsed_time_exp, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","print(\"Mean RMSLE for Global N-HiTS Model: {:.5f}.\".format(NHiTS_rmsle))\n","print('Training duration:', elapsed_time_nhits, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","print(\"Mean RMSLE for Global LSTM Model: {:.5f}.\".format(LSTM_rmsle))\n","print('Training duration:', elapsed_time_lstm, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","print(\"Mean RMSLE for Global TFT Model: {:.5f}.\".format(TFT_rmsle))\n","print('Training duration:', elapsed_time_tft, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","# print(\"Mean RMSLE for Global CatBoost Model{:.5f}.\".format(CatBoost_rmsle))\n","# print('Training duration:', elapsed_time_cboost, 'seconds')\n","# print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["But once again, those models are way from optimal, as they have not been trained with full data nor have their hyperparameters been tuned in a proper way, which requires a stronger machine (unless I coded the training process in a very inefficient way, then let me know please!).\n","\n","**Important:** Out of those global models, only TFT \"knows\" which series is which. It uses static covariates which contain info about store and product family, which identifies each series. N-HiTS, LSTM and LightGBM/CatBoost are trained with samples from all 1782 series without having direct information on the store/family ID of each series. That means, those models treat all samples as coming from the same data generating process. Whether that is a good assumption? Not sure - maybe it would work better to train global models for every product family or even store. There is always a trade-off between having more data and having more similarity between individual series for global models. I'm very interested in that topic - please comment if you have any ideas!"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4\"></a> <br>\n","# 5. Submission on Leaderboard"]},{"cell_type":"markdown","metadata":{},"source":["Re-trained the baseline Exponential Smoothing models on full training data leads to a **public RMSLE Score of 0.40578** - that's good enough to scratch the Top 10%. 33 Global LightGBM Models (1 for each product family) achieved a #1 **Leaderboard Score of 0.38558** at the time of submission. Additionally I added the code for generating submission forecasts with CatBoost, but with one global CatBoost model per product family. Uncomment for using any of them."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-19T22:03:14.26878Z","iopub.status.busy":"2022-09-19T22:03:14.267963Z","iopub.status.idle":"2022-09-19T22:03:14.276192Z","shell.execute_reply":"2022-09-19T22:03:14.275046Z","shell.execute_reply.started":"2022-09-19T22:03:14.268734Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","# Train Final Exponential Smoothing Models and Forecast for Submission\n","\n","ES_Models_Family_Dict_Submission = {}\n","ES_Forecasts_Family_Dict_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts for ts in sales_family]\n","\n","  ES_Models_Family_Dict_Submission[family] = ESModelBuilder(training_data)\n","  forecasts_ES = ESForecaster(ES_Models_Family_Dict_Submission[family])\n","    \n","  # Transform Back\n","  ES_Forecasts_Family_Dict_Submission[family] = family_pipeline_dict[family].inverse_transform(forecasts_ES, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(ES_Forecasts_Family_Dict_Submission[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          ES_Forecasts_Family_Dict_Submission[family][i] = ES_Forecasts_Family_Dict_Submission[family][i].map(lambda x: x * 0)\n","    \n","    \n","# Prepare Submission in Correct Format\n","\n","listofseries = []\n","\n","for store in range(0,54):\n","  for family in tqdm(family_list):\n","      oneforecast = ES_Forecasts_Family_Dict_Submission[family][store].pd_dataframe()\n","      oneforecast.columns = ['fcast']\n","      listofseries.append(oneforecast)\n","\n","df_forecasts = pd.concat(listofseries) \n","df_forecasts.reset_index(drop=True, inplace=True)\n","\n","# No Negative Forecasts\n","df_forecasts[df_forecasts < 0] = 0\n","forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n","\n","# Submission\n","submission_kaggle = forecasts_kaggle_sorted\n","submission_kaggle.to_csv('submission.csv', index=False)\n","\n","# Train 33 Global LightGBM Models with Full Data\n","\n","from sklearn.metrics import mean_squared_log_error as msle, mean_squared_error as mse\n","from lightgbm import early_stopping\n","\n","LGBM_Models_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  # Define Data for family\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts for ts in sales_family] \n","  TCN_covariates = future_covariates_dict[family]\n","  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n","\n","  LGBM_Model_Submission = LightGBMModel(lags = 63,\n","                             lags_future_covariates = (14,1),\n","                             lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22],\n","                             output_chunk_length=1,\n","                             random_state=2022,\n","                         #    max_bin= [63],\n","                             gpu_use_dp= \"false\")\n","     \n","  LGBM_Model_Submission.fit(series=train_sliced, \n","                        future_covariates=TCN_covariates,\n","                        past_covariates=transactions_transformed,\n","                        verbose=True)\n","\n","  LGBM_Models_Submission[family] = LGBM_Model_Submission\n","  \n","  # Generate Forecasts for Submission\n","\n","LGBM_Forecasts_Families_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts for ts in sales_family]\n","  LGBM_covariates = future_covariates_dict[family]\n","  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n","\n","  forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n","                                         series=train_sliced,\n","                                         future_covariates=LGBM_covariates,\n","                                         past_covariates=transactions_transformed)\n","  \n","  LGBM_Forecasts_Families_Submission[family] = forecast_LGBM\n","\n","# Transform Back\n","\n","LGBM_Forecasts_Families_back_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)\n","\n","# Zero Forecasting\n","\n","for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n","  for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n","    if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n","        LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n","        \n","# Prepare Submission in Correct Format\n","\n","listofseries = []\n","\n","for store in range(0,54):\n","  for family in tqdm(family_list):\n","      oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n","      oneforecast.columns = ['fcast']\n","      listofseries.append(oneforecast)\n","\n","df_forecasts = pd.concat(listofseries) \n","df_forecasts.reset_index(drop=True, inplace=True)\n","\n","# No Negative Forecasts\n","df_forecasts[df_forecasts < 0] = 0\n","forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n","\n","# Submission\n","submission_kaggle = forecasts_kaggle_sorted\n","submission_kaggle.to_csv('submission.csv', index=False)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\" \n","from darts.models import CatBoostModel\n","\n","def build_fit_family_cboost_model(\n","    lags,\n","    firstlag,\n","    pastcovlag,\n","    out_len,\n","    ):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # build the CatBoost model\n","    model = CatBoostModel(lags = lags,\n","                             lags_future_covariates = (firstlag,1),\n","                             lags_past_covariates = [-pastcovlag], \n","                             output_chunk_length=out_len,\n","                             learning_rate=learning_rate,\n","                             depth=depth,\n","                             early_stopping_rounds=10,\n","                             random_state=2022\n","                          )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-(val_len + lags) : ] for s in sales_family]\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_fam_shifted,\n","        val_past_covariates=only_past_covariates_fam_shifted,\n","        future_covariates=future_covariates_fam,\n","        val_future_covariates=future_covariates_fam\n","    )\n","\n","    return model\n","    \n","# Train Submission Model\n","\n","CatBoost_Models = {}\n","\n","for family in tqdm(family_list):\n","    \n","  sales_family = family_TS_transformed_dict[family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","\n","  # Split in train/val/test\n","  val_len = 16\n","  train = [s[: -val_len] for s in sales_family]\n","\n","  CatBoost_Model = build_fit_family_cboost_model( lags = 144,\n","    firstlag = 44,\n","    pastcovlag = 60,\n","    learning_rate = 0.06539829509538796,\n","    depth = 9,\n","    out_len = 3)\n","\n","  CatBoost_Models[family] = CatBoost_Model\n","  \n","# Generate Forecasts for Submission\n","\n","CatBoost_Forecasts_Families_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","                                                       \n","  forecast_CatBoost = CatBoost_Models[family].predict(n=16,\n","                                         series=training_data,\n","                                         future_covariates=future_covariates_fam,\n","                                         past_covariates=only_past_covariates_fam_shifted)\n","  \n","  CatBoost_Forecasts_Families_Submission[family] = forecast_CatBoost\n","\n","# Transform Back\n","\n","CatBoost_Forecasts_Families_back_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  CatBoost_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(CatBoost_Forecasts_Families_Submission[family], partial=True)\n","\n","# Zero Forecasting\n","\n","for family in tqdm(CatBoost_Forecasts_Families_back_Submission):\n","  for n in range(0,len(CatBoost_Forecasts_Families_back_Submission[family])):\n","    if (family_TS_dict[family][n].univariate_values()[-14:] == 0).all():\n","        CatBoost_Forecasts_Families_back_Submission[family][n] = CatBoost_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n","        \n","# Prepare Submission in Correct Format\n","\n","listofseries = []\n","\n","for store in range(0,54):\n","  for family in tqdm(family_list):\n","      oneforecast = CatBoost_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n","      oneforecast.columns = ['fcast']\n","      listofseries.append(oneforecast)\n","\n","df_forecasts = pd.concat(listofseries) \n","df_forecasts.reset_index(drop=True, inplace=True)\n","\n","# No Negative Forecasts\n","df_forecasts[df_forecasts < 0] = 0\n","forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n","\n","# Submission\n","submission_kaggle = forecasts_kaggle_sorted\n","submission_kaggle.to_csv('submission.csv', index=False)\n","\n","\"\"\""]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":2887556,"sourceId":29781,"sourceType":"competition"}],"dockerImageVersionId":30236,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
